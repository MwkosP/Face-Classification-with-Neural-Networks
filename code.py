# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVvFcXPS9WIM3DsaUbY3M9JHVFSvGglj

### ***DATASET SETUP***
"""

!pip install tensorflow
!pip install imagehash
!pip install tensorflow-addons

from google.colab import files
uploaded = files.upload()

import zipfile
import os
import numpy as np
import cv2
from sklearn.model_selection import train_test_split

# 1ï¸ Extract Dataset
zip_path = "/content/smallSet.zip"  # To Change name if needed
extract_path = "/content/smallSet"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

dataset_path = extract_path
print(f"Files extracted to: {extract_path}")

"""TRAIN - TEST SPLIT"""

# 2ï¸ Load Images into X (Features) and y (Labels)
image_size = (299, 299)  # â¬… Change this

X, y = [], []

# Define class labels
class_labels = {"fake": 0, "real": 1}

# Loop through dataset
for class_name, class_index in class_labels.items():
    class_path = os.path.join(dataset_path, class_name)

    for img_name in os.listdir(class_path):
        img_path = os.path.join(class_path, img_name)
        img = cv2.imread(img_path)
        img = cv2.resize(img, image_size)
        X.append(img)
        y.append(class_index)

# Convert to NumPy arrays
X = np.array(X) / 255.0  # Normalize images
y = np.array(y)

print(f"Total images loaded: {len(X)}")
print(f"Class distribution: {np.bincount(y)}")  # Check balance

# 3ï¸ Split into Train (80%), Validation (10%), Test (10%)
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)

#  FIX: Correct stratification in second split
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1111, random_state=42, stratify=y_trainval)

print(f"Train set: {len(X_train)} images ({len(X_train)/len(X)*100:.1f}%)")
print(f"Validation set: {len(X_val)} images ({len(X_val)/len(X)*100:.1f}%)")
print(f"Test set: {len(X_test)} images ({len(X_test)/len(X)*100:.1f}%)")

"""ENSURE BALANCED DATASET"""

# check class distribution
def check_balance(y_split, split_name):
    unique, counts = np.unique(y_split, return_counts=True)
    balance = {int(k): int(v) for k, v in zip(unique, counts)}  # Convert np.int64 â†’ int
    print(f"{split_name} class distribution:", balance)


check_balance(y_train, "Train")
check_balance(y_val, "Validation")
check_balance(y_test, "Test")

"""SUBFOLDER STRUCTURE INTEGRITY

CHECK IF SUBFOLDERS HAVE CORRECT FORM
import os

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Ï„Ï‰Î½ Ï…Ï€Î¿Ï†Î±ÎºÎ­Î»Ï‰Î½ Ï„Î¿Ï… dataset
for subdir, dirs, files in os.walk(extract_path):
    print(f"Subfolder: {subdir}")
    print(f"Files: {files}")
-------------------------------------------------------------

CORRECT FORM:

/content/smallSet/
    real/
        image1.jpg
        image2.jpg
        ...
    fake/
        image1.jpg
        image2.jpg
        ...

# ***Data Preprocessing & Exploration***

View 5 Images of each Class
"""

import matplotlib.pyplot as plt
import os
import numpy as np

def display_images(dataset_split):
    # Choose the correct dataset based on input
    if dataset_split == 'train':
        X_data, y_data = X_train, y_train
        fake_indices = [i for i in range(len(y_train)) if y_train[i] == 0]
        real_indices = [i for i in range(len(y_train)) if y_train[i] == 1]
        fake_images = X_train[fake_indices[:5]]
        real_images = X_train[real_indices[:5]]
        fake_filenames = [os.path.basename(img_name) for img_name in np.array(os.listdir(os.path.join(dataset_path, 'fake')))[fake_indices[:5]]]
        real_filenames = [os.path.basename(img_name) for img_name in np.array(os.listdir(os.path.join(dataset_path, 'real')))[real_indices[:5]]]
    elif dataset_split == 'validation':
        X_data, y_data = X_val, y_val
        fake_indices = [i for i in range(len(y_val)) if y_val[i] == 0]
        real_indices = [i for i in range(len(y_val)) if y_val[i] == 1]
        fake_images = X_val[fake_indices[:5]]
        real_images = X_val[real_indices[:5]]
        fake_filenames = [os.path.basename(img_name) for img_name in np.array(os.listdir(os.path.join(dataset_path, 'fake')))[fake_indices[:5]]]
        real_filenames = [os.path.basename(img_name) for img_name in np.array(os.listdir(os.path.join(dataset_path, 'real')))[real_indices[:5]]]
    elif dataset_split == 'test':
        X_data, y_data = X_test, y_test
        fake_indices = [i for i in range(len(y_test)) if y_test[i] == 0]
        real_indices = [i for i in range(len(y_test)) if y_test[i] == 1]
        fake_images = X_test[fake_indices[:5]]
        real_images = X_test[real_indices[:5]]
        fake_filenames = [os.path.basename(img_name) for img_name in np.array(os.listdir(os.path.join(dataset_path, 'fake')))[fake_indices[:5]]]
        real_filenames = [os.path.basename(img_name) for img_name in np.array(os.listdir(os.path.join(dataset_path, 'real')))[real_indices[:5]]]
    else:
        print("Invalid dataset split. Please choose 'train', 'validation', or 'test'.")
        return

    # Plot the images
    plt.figure(figsize=(12, 6))  # Set figure size for better visualization

    # Plot fake images
    for i, (fake_img, fake_filename) in enumerate(zip(fake_images, fake_filenames)):
        plt.subplot(2, 5, i+1)  # Plot in the first row for fake images
        plt.imshow(fake_img)
        plt.axis('off')
        plt.title(f"{fake_filename}", fontsize=10)  # Only show the filename on the image

    # Plot real images
    for i, (real_img, real_filename) in enumerate(zip(real_images, real_filenames)):
        plt.subplot(2, 5, i+6)  # Plot in the second row for real images
        plt.imshow(real_img)
        plt.axis('off')
        plt.title(f"{real_filename}", fontsize=10)  # Only show the filename on the image

    # Set the title for the entire figure
    plt.suptitle(f"{dataset_split.capitalize()} Set: Fake and Real Images", fontsize=16, y=1.05)

    plt.tight_layout()  # Adjust layout to prevent overlap
    plt.subplots_adjust(top=0.85)  # Adjust space for the title
    plt.show()

display_images('train')
display_images('test')
display_images('validation')

import numpy as np
import cv2
import tensorflow as tf
import random
import matplotlib.pyplot as plt

# Define augmentation functions:
def random_flip(image):
    return tf.image.random_flip_left_right(image)

def random_rotation(image, max_angle=40):
    angle = random.uniform(-max_angle, max_angle)
    return tf.image.rot90(image, k=int(angle // 90))  # Rotate image by 90, 180, or 270 degrees

def random_zoom(image, zoom_factor=0.2):
    height, width, _ = image.shape
    zoomed = tf.image.resize(image, (int(height * (1 + zoom_factor)), int(width * (1 + zoom_factor))))
    return zoomed

def random_shift(image, shift_range=0.2):
    height, width, _ = image.shape
    shift_height = int(shift_range * height)
    shift_width = int(shift_range * width)
    shifted_image = tf.image.random_crop(image, size=[height - shift_height, width - shift_width, 3])
    return shifted_image

def random_brightness(image):
    return tf.image.random_brightness(image, max_delta=0.2)

def random_contrast(image):
    return tf.image.random_contrast(image, lower=0.7, upper=1.3)

# Combine augmentations
def augment_image(image):
    image = random_flip(image)
    image = random_rotation(image)
    image = random_zoom(image)
    image = random_shift(image)
    image = random_brightness(image)
    image = random_contrast(image)

    image = tf.image.resize(image, [299, 299])         # Resize to InceptionV3 input


    return image

# Show some augmented images
plt.figure(figsize=(10, 10))
for i in range(5):
    plt.subplot(1, 5, i+1)
    augmented_img = augment_image(X_train[i])  # Augment the first 5 training images

    plt.imshow(augmented_img)
    plt.axis('off')
    plt.title(f"Train set Augmented {i+1}")

plt.tight_layout()
plt.show()

# Optionally, you can apply augmentation to the entire training set
X_train_augmented = np.array([augment_image(img) for img in X_train])

"""LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import matplotlib.pyplot as plt

# Flatten training images for LDA
flat_images = X_train.reshape(X_train.shape[0], -1)

# Apply LDA
lda = LDA(n_components=1)  # Only n_classes - 1 components are possible, so here it's 1
lda_result = lda.fit_transform(flat_images, y_train)

plt.figure(figsize=(8, 6))
plt.scatter(lda_result[:, 0], np.zeros_like(lda_result[:, 0]), c=y_train, cmap="coolwarm", alpha=0.7)
plt.colorbar(label="0: Fake, 1: Real")
plt.xlabel("Linear Discriminant 1")
plt.title("LDA Visualization of Image Dataset (Train Set)")
plt.yticks([])  # Since LDA gives 1D result, remove y-axis ticks
plt.show()

"""** t-SNE**"""

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
tsne_result = tsne.fit_transform(flat_images)

plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=y_train, cmap='coolwarm', alpha=0.7)
plt.title("t-SNE Visualization")
plt.colorbar(label='0: Fake, 1: Real')
plt.show()

"""Class-wise mean image"""

mean_fake = np.mean(X_train[y_train == 0], axis=0)
mean_real = np.mean(X_train[y_train == 1], axis=0)

plt.subplot(1, 2, 1)
plt.imshow(mean_fake)
plt.title("Mean Fake Image")
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(mean_real)
plt.title("Mean Real Image")
plt.axis('off')

plt.show()

"""Class distribution"""

import seaborn as sns
sns.countplot(x=y_train)
plt.title("Class Distribution in Training Set")

"""pixel variance"""

variances = [np.var(img) for img in X_train]
plt.hist(variances, bins=50)
plt.title("Pixel Variance Distribution")

"""Check for Duplicates"""

import os
import imagehash
from PIL import Image
from skimage.metrics import structural_similarity as ssim
import numpy as np
from collections import defaultdict
from itertools import combinations

# ğŸ”§ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚
#dataset_path   # â† Î¬Î»Î»Î±Î¾Î­ Ï„Î¿ Î¼Îµ Ï„Î¿ ÏƒÏ‰ÏƒÏ„ÏŒ path
phash_threshold = 5                # Î ÏŒÏƒÎ¿ ÎºÎ¿Î½Ï„Î¬ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Ï„Î± perceptual hashes
ssim_threshold = 0.90              # SSIM Î³Î¹Î± Î½Î± Î¸ÎµÏ‰ÏÎ·Î¸Î¿ÏÎ½ Ï€Î±ÏÏŒÎ¼Î¿Î¹ÎµÏ‚

# ğŸ“Œ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ phash Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎµÎ¹ÎºÏŒÎ½Î± ÏƒÏ„Î¿Ï…Ï‚ Ï…Ï€Î¿Ï†Î±ÎºÎ­Î»Î¿Ï…Ï‚
hash_dict = defaultdict(list)

for root, _, files in os.walk(dataset_path):
    for filename in files:
        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
            path = os.path.join(root, filename)
            try:
                img = Image.open(path).convert("RGB").resize((128, 128))
                h = imagehash.phash(img)
                hash_dict[str(h)].append((h, path))
            except:
                continue

# ğŸ“Œ ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÎ¿Î½Ï„Î¹Î½ÏÎ½ hashes
potential_duplicates = []
paths = [item for sublist in hash_dict.values() for item in sublist]

print(f"ğŸ” Î•Î¹ÎºÏŒÎ½ÎµÏ‚ Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿: {len(paths)}")
print("ğŸ§  ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Ï€Î¹Î¸Î±Î½Î­Ï‚ Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€ÎµÏ‚ ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚...")

for (h1, path1), (h2, path2) in combinations(paths, 2):
    if h1 - h2 <= phash_threshold:
        try:
            img1 = np.array(Image.open(path1).convert('L').resize((128, 128)))
            img2 = np.array(Image.open(path2).convert('L').resize((128, 128)))
            score, _ = ssim(img1, img2, full=True)
            if score >= ssim_threshold:
                potential_duplicates.append((path1, path2, score))
        except:
            continue

# ğŸ“Œ Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
if potential_duplicates:
    print(f"\nğŸ”— Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(potential_duplicates)} Ï€Î¹Î¸Î±Î½Î­Ï‚ Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€ÎµÏ‚ ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚:")
    for p1, p2, score in potential_duplicates:
        print(f"\nâœ… SSIM: {score:.3f}")
        print(f"ğŸ“ {p1}")
        print(f"ğŸ“ {p2}")
else:
    print("âœ… Î”ÎµÎ½ ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÏ„Î·ÎºÎ±Î½ Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€ÎµÏ‚ ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚ Î¼Îµ Ï„Î± ÎºÏÎ¹Ï„Î®ÏÎ¹Î± Ï€Î¿Ï… Î¿ÏÎ¯ÏƒÏ„Î·ÎºÎ±Î½.")

"""# ***Architecture***

Xception
"""

from tensorflow.keras.applications import Xception
from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model

# Load Xception without the top layers (pretrained on ImageNet)
xception_base = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))

# Freeze the layers
for layer in xception_base.layers:
    layer.trainable = False

# Add custom layers on top of the base model
x = GlobalAveragePooling2D()(xception_base.output)  # better than Flatten for CNNs
x = Dense(256, activation='relu')(x)
x = Dense(1, activation='sigmoid')(x)  # binary classification: fake vs real

# Final model
Model_Xception = Model(inputs=xception_base.input, outputs=x)

# Print the model summary
Model_Xception.summary()
plot_model(Model_Xception, show_shapes=True, show_layer_names=True, dpi=50, rankdir="LR")  # LR = Left to Right (Î¿ÏÎ¹Î¶ÏŒÎ½Ï„Î¹Î±)

"""***InceptionV3***"""

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model


# Load InceptionV3 without top layers
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))

# Freeze the layers of InceptionV3
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers on top
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification

# Final model
Model_InceptionV3 = Model(inputs=base_model.input, outputs=x)

# Print the model summary
Model_InceptionV3.summary()
plot_model(Model_InceptionV3, show_shapes=True, show_layer_names=True, dpi=50, rankdir="LR")  # LR = Left to Right (Î¿ÏÎ¹Î¶ÏŒÎ½Ï„Î¹Î±)

"""EfficientNetB7"""

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model

# Input shape
IMG_SIZE = 299

# Load EfficientNetB7 without top layers
base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))

# Custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(1, activation='sigmoid')(x)

# Define model
Model_EfficientNetB7 = Model(inputs=base_model.input, outputs=output)

"""DenseNet121"""

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Input

# Assuming your images are resized to 224x224
input_shape = (299, 299, 3)
base_model = DenseNet121(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))

# Freeze base model if you want to use it as a fixed feature extractor first
base_model.trainable = False  # You can later unfreeze for fine-tuning

# Add custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

# Final model
Model_DenseNet121 = Model(inputs=base_model.input, outputs=output)

# Compile the model
Model_DenseNet121.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

"""CNN"""

from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import Model, Sequential


# Define image dimensions based on the earlier image_size
img_height, img_width = image_size # Use the existing image_size variable


# Model architecture
Model_CNN = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid'),  # Binary classification output
])

"""# ***Train***"""

from tensorflow.keras.callbacks import EarlyStopping

bs=8
epochs=100
early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)

"""Train Xception"""

Model_Xception.compile(optimizer=Adam(learning_rate=1e-4),
                       loss='binary_crossentropy',
                       metrics=['accuracy']
)

history_Xception = Model_Xception.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=bs,
    validation_data=(X_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)

test_loss, test_acc = Model_Xception.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""***Train InceptionV3***"""

from tensorflow.keras.optimizers import Adam


Model_InceptionV3.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

history_InceptionV3 = Model_InceptionV3.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=bs,
    callbacks=[early_stopping]
)


test_loss, test_acc = Model_InceptionV3.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_InceptionV3.save('InceptionV3.h5')  # Save in HDF5 format (popular, portable)

"""Train EfficientNetB7"""

Model_EfficientNetB7.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

history_EfficientNetB7 = Model_EfficientNetB7.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=bs,
    callbacks=[early_stopping]
)

test_loss, test_acc = Model_EfficientNetB7.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_EfficientNetB7.save('EfficientNetB7.h5')  # Save in HDF5 format (popular, portable)

"""Train DenseNet121"""

Model_DenseNet121.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

history_DenseNet121 = Model_DenseNet121.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=bs,
    validation_data=(X_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)


test_loss, test_acc = Model_DenseNet121.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_DenseNet121.save('DenseNet121.h5')  # Save in HDF5 format (popular, portable)

"""CNN"""

from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
Model_CNN.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

history_CNN = Model_CNN.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=bs,
    validation_data=(X_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)


test_loss, test_acc = Model_CNN.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_CNN.save('CNN.h5')  # Save in HDF5 format (popular, portable)

"""# ***Evaluate***

Evaluate Xception
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix


pred_probs = Model_Xception.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()


true_labels = y_val.flatten()


print(classification_report(true_labels, pred_labels, target_names=class_labels))


cm = confusion_matrix(true_labels, pred_labels)


plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize Xception Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_Xception.history['accuracy'], label='Train Accuracy')
plt.plot(history_Xception.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_Xception.history['loss'], label='Train Loss', color='red')
plt.plot(history_Xception.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

"""Evaluate InceptionV3"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

pred_probs = Model_InceptionV3.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()  # for binary classification

true_labels = y_val.flatten()

print(classification_report(true_labels, pred_labels, target_names=class_labels))

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize InceptionV3 Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_InceptionV3.history['accuracy'], label='Train Accuracy')
plt.plot(history_InceptionV3.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_InceptionV3.history['loss'], label='Train Loss', color='red')
plt.plot(history_InceptionV3.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

"""Evaluate EfficientNetB7"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

pred_probs = Model_EfficientNetB7.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()  # for binary classification

true_labels = y_val.flatten()

print(classification_report(true_labels, pred_labels, target_names=class_labels))

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize InceptionV3 Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_EfficientNetB7.history['accuracy'], label='Train Accuracy')
plt.plot(history_EfficientNetB7.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_EfficientNetB7.history['loss'], label='Train Loss', color='red')
plt.plot(history_EfficientNetB7.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

"""Evaluate DenseNet121"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix


pred_probs = Model_DenseNet121.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()


true_labels = y_val.flatten()


print(classification_report(true_labels, pred_labels, target_names=class_labels))


cm = confusion_matrix(true_labels, pred_labels)


plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize DenseNet121 Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_DenseNet121.history['accuracy'], label='Train Accuracy')
plt.plot(history_DenseNet121.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_DenseNet121.history['loss'], label='Train Loss', color='red')
plt.plot(history_DenseNet121.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

"""Evaluate CNN"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix


pred_probs = Model_CNN.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()


true_labels = y_val.flatten()


print(classification_report(true_labels, pred_labels, target_names=class_labels))


cm = confusion_matrix(true_labels, pred_labels)


plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize CNN Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_CNN.history['accuracy'], label='Train Accuracy')
plt.plot(history_CNN.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_CNN.history['loss'], label='Train Loss', color='red')
plt.plot(history_CNN.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

"""# ***K-Fold***

Kfold made into a function fit for eery Model.
"""

from sklearn.model_selection import KFold
from tensorflow.keras import backend as K



def kFoldFunction(model,k):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    train_accuracies = []
    val_accuracies = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        K.clear_session()
        print(f"\nFold {fold + 1}/{k}")

        # Clone the model architecture and compile again
        cloned_model = tf.keras.models.clone_model(model)
        cloned_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

        X_train_kfold, X_val_kfold = X[train_idx], X[val_idx]
        y_train_kfold, y_val_kfold = y[train_idx], y[val_idx]

        cloned_model.fit(
            X_train_kfold, y_train_kfold,
            validation_data=(X_val_kfold, y_val_kfold),
            epochs=epochs, batch_size=bs, verbose=1
        )

        train_loss, train_acc = cloned_model.evaluate(X_train_kfold, y_train_kfold, verbose=0)
        val_loss, val_acc = cloned_model.evaluate(X_val_kfold, y_val_kfold, verbose=0)

        print(f"Fold {fold + 1} Train Accuracy: {train_acc:.4f}")
        print(f"Fold {fold + 1} Validation Accuracy: {val_acc:.4f}")

        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

    print(f"\nFinal Mean Train Accuracy: {np.mean(train_accuracies):.4f}")
    print(f"Final Mean Validation Accuracy: {np.mean(val_accuracies):.4f}")

k=3

print(f"KFold on Xception ")
kFoldFunction(Model_Xception,k) #Xception

print(f"\nKFold on Inceptionv3 ")
kFoldFunction(Model_InceptionV3,k) #InceptionV3

print(f"\nKFold on EfficientNetB7 \n")
kFoldFunction(Model_EfficientNetB7,k) #EfficientNetB7

print(f"\nKFold on DenseNet121 ")
kFoldFunction(Model_DenseNet121,k) #EfficientNetB7

print(f"\nKFold on CNN ")
kFoldFunction(Model_CNN,k) #EfficientNetB7

"""# ***Fine-Tune***

***Xception***
"""

from tensorflow.keras.applications import Xception
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

# ÎÎµÎºÎ»ÎµÎ¯Î´Ï‰ÏƒÎµ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î® Î»Î¹Î³ÏŒÏ„ÎµÏÎ± layers
for layer in xception_base.layers[-500:]:  # <-- Î¬Î»Î»Î±Î¾Îµ Ï„Î¿Î½ Î±ÏÎ¹Î¸Î¼ÏŒ ÎµÎ´Ï (Ï€.Ï‡. 5, 10, 20)
    layer.trainable = True

#Isws min xreiastei
# Add custom top with Dropout and L2 regularization
x = GlobalAveragePooling2D()(xception_base.output)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)
x = Dropout(0.4)(x)  # Drop 40% of nodes randomly during training
output = Dense(1, activation='sigmoid', kernel_regularizer=l2(1e-4))(x)

# Final model
Model_Xception = Model(inputs=xception_base.input, outputs=output)

"""***Inceptionv3***"""

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Activation


# Load InceptionV3 without the top layers
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))

# ÎÎµÎºÎ»ÎµÎ¯Î´Ï‰ÏƒÎµ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î® Î»Î¹Î³ÏŒÏ„ÎµÏÎ± layers
for layer in base_model.layers[-300:]:  # <-- Î¬Î»Î»Î±Î¾Îµ Ï„Î¿Î½ Î±ÏÎ¹Î¸Î¼ÏŒ ÎµÎ´Ï (Ï€.Ï‡. 5, 10, 20)
    layer.trainable = True


#Isws min xreiastei
# Add custom top with Dropout and L2 regularization
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)
x = Dropout(0.4)(x)  # Drop 40% of nodes randomly during training
output = Dense(1, activation='sigmoid', kernel_regularizer=l2(1e-4))(x)

# Model
Model_InceptionV3 = Model(inputs=base_model.input, outputs=output)

"""***EfficientNetB7***"""

from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# Load EfficientNetB7 without the top layers
base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(299, 299, 3))

# Unfreeze last few layers (fine-tune deeper as needed)
for layer in base_model.layers[-300:]:
    layer.trainable = True

# Add custom top
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)
x = Dropout(0.4)(x)
output = Dense(1, activation='sigmoid', kernel_regularizer=l2(1e-4))(x)

# Build model
Model_EfficientNetB7 = Model(inputs=base_model.input, outputs=output)

"""DenseNet121"""

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# Load DenseNet121 without top layers
# Use 299x299 if your dataset is already prepared at that size
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(299, 299, 3))


# Fine-tune last layers (you can adjust how many layers to unfreeze)
for layer in base_model.layers[-150:]:  # Tune this value based on your task and resources
    layer.trainable = True

# Add custom top layers
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)
x = Dropout(0.4)(x)
output = Dense(1, activation='sigmoid', kernel_regularizer=l2(1e-4))(x)

# Build model
Model_DenseNet121 = Model(inputs=base_model.input, outputs=output)

"""CNN"""

from tensorflow.keras.layers import Flatten

Model_CNN = Sequential([
    Conv2D(32, (3,3), padding='same', input_shape=(img_height, img_width, 3)),
    BatchNormalization(),
    Activation('relu'),
    MaxPooling2D(2,2),
    Dropout(0.25),

    Conv2D(64, (3,3), padding='same'),
    BatchNormalization(),
    Activation('relu'),
    MaxPooling2D(2,2),
    Dropout(0.3),

    Conv2D(128, (3,3), padding='same'),
    BatchNormalization(),
    Activation('relu'),
    MaxPooling2D(2,2),
    Dropout(0.4),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid'),
])

"""# ***Retrain***

***Xception***
"""

Model_Xception.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history_Xception_ft = Model_Xception.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=bs,
    validation_data=(X_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)

test_loss, test_acc = Model_Xception.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_Xception.save('Xception.h5')  # Save in HDF5 format (popular, portable)

"""Inceptionv3"""

from tensorflow.keras.optimizers import Adam

# Compile the model with SGD + Momentum
Model_InceptionV3.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train the model
history_Inceptionv3_ft = Model_InceptionV3.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=bs,
    validation_data=(X_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)

# Evaluate the model
test_loss, test_acc = Model_InceptionV3.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_InceptionV3.save('InceptionV3.h5')  # Save in HDF5 format (popular, portable)

"""***TRAIN EfficientNetB7***"""

# Recompile with lower learning rate
Model_EfficientNetB7.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train Stage 2
history_EfficientNetB7_ft = Model_EfficientNetB7.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=bs,
   callbacks=[early_stopping]
)


test_loss, test_acc = Model_EfficientNetB7.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_EfficientNetB7.save('EfficientNetB7.h5')  # Save in HDF5 format (popular, portable)

"""***Train DenseNet121***"""

# Recompile with lower learning rate
Model_DenseNet121.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train Stage 2
history_DenseNet121_ft = Model_DenseNet121.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=bs,
   callbacks=[early_stopping]
)


test_loss, test_acc = Model_DenseNet121.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

Model_DenseNet121.save('DenseNet121.h5')  # Save in HDF5 format (popular, portable)

"""CNN"""

Model_CNN.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history_Model_CNN_ft = Model_CNN.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=bs,
    validation_data=(X_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)

# Evaluate on test set
test_loss, test_acc = Model_CNN.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""# ***ReEvaluate***

Evaluate CNN FINE TUNED Model
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

pred_probs = Model_CNN1.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()

true_labels = y_val.flatten()

print(classification_report(true_labels, pred_labels, target_names=class_labels))

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize CNN FINE TUNED Training Progress"""

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history_cnn_ft.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn_ft.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history_cnn_ft.history['loss'], label='Train Loss', color='red')
plt.plot(history_cnn_ft.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

Model_CNN1.save("CNNModelFineTuned.h5")  # Saves as an HDF5 file

"""Evaluate VGG16 FINE TUNED Model"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

pred_probs = Model_VGG16.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()  # for binary classification

true_labels = y_val.flatten()

print(classification_report(true_labels, pred_labels, target_names=class_labels))

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize VGG16 FINE TUNED Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_vgg16_ft.history['accuracy'], label='Train Accuracy')
plt.plot(history_vgg16_ft.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_vgg16_ft.history['loss'], label='Train Loss', color='red')
plt.plot(history_vgg16_ft.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

Model_VGG16.save("VGG16FineTuned.keras")

"""***evaluate ResNet***"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

pred_probs = Model_ResNet50.predict(X_val)
pred_labels = (pred_probs > 0.5).astype(int).flatten()

true_labels = y_val.flatten()

print(classification_report(true_labels, pred_labels, target_names=class_labels))

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""Visualize ResNet Training Progress"""

import matplotlib.pyplot as plt

plt.plot(history_resnet_ft.history['accuracy'], label='Train Accuracy')
plt.plot(history_resnet_ft.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

plt.plot(history_resnet_ft.history['loss'], label='Train Loss', color='red')
plt.plot(history_resnet_ft.history['val_loss'], label='Validation Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

Model_ResNet50.save("RESNET50FineTuned.keras")

"""# ***AoV***"""

from scipy.stats import f_oneway

xception = [0.64, 0.65, 0.66]
inception = [0.72, 0.73, 0.74]
efficientnet = [0.49, 0.50, 0.48]
densenet = [0.62, 0.63, 0.64]
cnn = [0.58, 0.59, 0.57]

f_stat, p_val = f_oneway(xception, inception, efficientnet, densenet, cnn)

print(f"F-statistic: {f_stat:.4f}")
print(f"p-value: {p_val:.4f}")
